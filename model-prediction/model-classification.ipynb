{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# organize imports\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# keras imports\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "with open('conf.json') as f:    \n",
    "  config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "model_name    = config[\"model\"]\n",
    "weights     = config[\"weights\"]\n",
    "include_top   = config[\"include_top\"]\n",
    "train_path    = config[\"train_path\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "test_size     = config[\"test_size\"]\n",
    "results     = config[\"results\"]\n",
    "model_path    = config[\"model_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time\n",
    "print (\"[STATUS] start time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "start = time.time()\n",
    "\n",
    "# create the pretrained models\n",
    "# check for pretrained weight usage or not\n",
    "# check for top layers to be included or not\n",
    "base_model = InceptionV3(include_top=include_top, weights=weights, input_tensor=Input(shape=(299,299,3)))\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('custom').output)\n",
    "image_size = (299, 299)\n",
    "\n",
    "print (\"[INFO] successfully loaded base model and model...\")\n",
    "\n",
    "# path to training dataset\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# encode the labels\n",
    "print (\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "features = []\n",
    "labels   = []\n",
    "\n",
    "# loop over all the labels in the folder\n",
    "count = 1\n",
    "for i, label in enumerate(train_labels):\n",
    "  cur_path = train_path + \"/\" + label\n",
    "  count = 1\n",
    "  for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features.append(flat)\n",
    "    labels.append(label)\n",
    "    #print (\"[INFO] processed - \" + str(count))\n",
    "    count += 1\n",
    "  print (\"[INFO] completed label - \" + label)\n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "# get the shape of training labels\n",
    "#print (\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print (\"[STATUS] training labels shape: {}\".format(le_labels.shape))\n",
    "\n",
    "# save features and labels\n",
    "h5f_data = h5py.File(features_path, 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(features))\n",
    "\n",
    "h5f_label = h5py.File(labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(le_labels))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(model_path + str(test_size) + \".json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "\n",
    "# save weights\n",
    "model.save_weights(model_path + str(test_size) + \".h5\")\n",
    "print(\"[STATUS] saved model and weights to disk..\")\n",
    "\n",
    "print (\"[STATUS] features and labels saved..\")\n",
    "\n",
    "# end time\n",
    "end = time.time()\n",
    "print (\"[STATUS] end time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "\n",
    "#Training Machine Learning algorithm\n",
    "\n",
    "# load the user configs\n",
    "with open('conf.json') as f:    \n",
    "  config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "seed      = config[\"seed\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "results     = config[\"results\"]\n",
    "classifier_path = config[\"classifier_path\"]\n",
    "train_path    = config[\"train_path\"]\n",
    "num_classes   = config[\"num_classes\"]\n",
    "classifier_path = config[\"classifier_path\"]\n",
    "\n",
    "# import features and labels\n",
    "h5f_data  = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset_1']\n",
    "labels_string   = h5f_label['dataset_1']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# verify the shape of features and labels\n",
    "print (\"[INFO] features shape: {}\".format(features.shape))\n",
    "print (\"[INFO] labels shape: {}\".format(labels.shape))\n",
    "\n",
    "print (\"[INFO] training started...\")\n",
    "# split the training and validation data\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(np.array(features),\n",
    "                                                                  np.array(labels),\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=seed)\n",
    "\n",
    "print (\"[INFO] splitted train and test data...\")\n",
    "print (\"[INFO] train data  : {}\".format(trainData.shape))\n",
    "print (\"[INFO] val data   : {}\".format(valData.shape))\n",
    "print (\"[INFO] train labels: {}\".format(trainLabels.shape))\n",
    "print (\"[INFO] val labels : {}\".format(valLabels.shape))\n",
    "\n",
    "# use SVM as the model\n",
    "print (\"[INFO] creating model...\")\n",
    "\n",
    "classifier = SVC(probability=True)\n",
    "\n",
    "classifier.fit(trainData, trainLabels)\n",
    "\n",
    "# use rank-1 and rank-3 predictions\n",
    "print (\"[INFO] evaluating model...\")\n",
    "f = open(results, \"w\")\n",
    "rank_1 = 0\n",
    "rank_3 = 0\n",
    "\n",
    "# loop over test data\n",
    "for (label, features) in zip(valLabels, valData):\n",
    "  # predict the probability of each class label and\n",
    "  # take the top-5 class labels\n",
    "  predictions = classifier.predict_proba(np.atleast_2d(features))[0]\n",
    "  predictions = np.argsort(predictions)[::-1][:5]\n",
    "\n",
    "  # rank-1 prediction increment\n",
    "  if label == predictions[0]:\n",
    "    rank_1 += 1\n",
    "\n",
    "  # rank-3 prediction increment\n",
    "  if label in predictions[:3]:\n",
    "    rank_3 += 1\n",
    "\n",
    "# convert accuracies to percentages\n",
    "rank_1 = (rank_1 / float(len(valLabels))) * 100\n",
    "rank_3 = (rank_3 / float(len(valLabels))) * 100\n",
    "\n",
    "# write the accuracies to file\n",
    "f.write(\"Rank-1: {:.2f}%\\n\".format(rank_1))\n",
    "f.write(\"Rank-3: {:.2f}%\\n\\n\".format(rank_3))\n",
    "\n",
    "\n",
    "# evaluate the model of train data\n",
    "preds_train = classifier.predict(trainData)\n",
    "\n",
    "# evaluate the model of test data\n",
    "preds_val = classifier.predict(valData)\n",
    "\n",
    "# write the classification report to file\n",
    "f.write(\"{}\\n\".format(classification_report(valLabels, preds_val)))\n",
    "f.close()\n",
    "\n",
    "# dump classifier to file\n",
    "print (\"[INFO] saving model...\")\n",
    "pickle.dump(classifier, open(classifier_path, 'wb'))\n",
    "\n",
    "# display the confusion matrix\n",
    "print (\"[INFO] confusion matrix\")\n",
    "\n",
    "# get the list of training lables\n",
    "labels = sorted(list(os.listdir(train_path)))\n",
    "\n",
    "# plot the confusion matrix\n",
    "cm = confusion_matrix(valLabels, preds_val)\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            cmap=\"Set2\")\n",
    "plt.show()\n",
    "\n",
    "train_accuracy = accuracy_score(trainLabels, preds_train)\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.write(\"The training accuracy is:\" + str(train_accuracy) + '\\n\\n')\n",
    "    f.write(\"The actual labels are:\" + str([train_labels[idx] for idx in trainLabels]) + '\\n\\n')\n",
    "    f.write(\"The predicted labels are:\" + str([train_labels[idx] for idx in preds_train]) + '\\n\\n')\n",
    "\n",
    "val_accuracy = accuracy_score(valLabels, preds_val)\n",
    "with open('val.txt', 'w') as f:\n",
    "    f.write(\"The validation accuracy is:\" + str(val_accuracy) + '\\n\\n')\n",
    "    f.write(\"The actual labels are:\" + str([train_labels[idx] for idx in valLabels]) + '\\n\\n')\n",
    "    f.write(\"The predicted labels are:\" + str([train_labels[idx] for idx in preds_val]) + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
