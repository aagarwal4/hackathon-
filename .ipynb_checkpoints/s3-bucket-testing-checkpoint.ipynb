{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# organize imports\n",
    "from __future__ import print_function\n",
    "\n",
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "#imports\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# keras imports\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "\n",
    "# S3 imports\n",
    "from io import BytesIO\n",
    "import boto3, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../rootkey.csv\") as f:\n",
    "    ACCESS_ID = f.readline().strip().split('=')[1]\n",
    "    ACCESS_KEY = f.readline().strip().split('=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', \n",
    "                    aws_access_key_id=ACCESS_ID,\n",
    "                    aws_secret_access_key= ACCESS_KEY)\n",
    "\n",
    "myBucket = s3.Bucket('hackathon-nissan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BytesIO() as data:\n",
    "    myBucket.download_fileobj(\"classifier.pickle\", data)\n",
    "    data.seek(0)    # move back to the beginning after writing\n",
    "    classifier = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the loaded classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"cu...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# get all the train labels\n",
    "train_labels = [\"scratch\", \"no-scratch\"]\n",
    "base_model = InceptionV3(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299,299,3)))\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('custom').output)\n",
    "image_size = (299, 299)\n",
    "img = image.load_img(\"/home/ec2-user/damage360/datasets/demo-photos/damaged/20181005_165421.jpg\", target_size=image_size)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "feature = model.predict(x)\n",
    "flat = feature.flatten()\n",
    "flat = np.expand_dims(flat, axis=0)\n",
    "preds = classifier.predict(flat)\n",
    "prediction = train_labels[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scratch'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading file to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Python variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "import boto\n",
    "from filechunkio import FileChunkIO\n",
    "\n",
    "# Connect to S3\n",
    "s3_connection = boto.connect_s3(aws_access_key_id=ACCESS_ID, aws_secret_access_key=ACCESS_KEY)\n",
    "bucket = s3_connection.get_bucket('hackathon-nissan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file info\n",
    "source_path = 'model-prediction/features-models.h5'\n",
    "source_size = os.stat(source_path).st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multipart upload request\n",
    "mp = bucket.initiate_multipart_upload(os.path.basename(source_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a chunk size of 50 MiB (feel free to change this)\n",
    "chunk_size = 52428800\n",
    "chunk_count = int(math.ceil(source_size / float(chunk_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompleteMultiPartUpload: hackathon-nissan.features-models.h5>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the file parts, using FileChunkIO to create a file-like object\n",
    "# that points to a certain byte range within the original file. We\n",
    "# set bytes to never exceed the original file size.\n",
    "for i in range(chunk_count):\n",
    "    offset = chunk_size * i\n",
    "    bytes = min(chunk_size, source_size - offset)\n",
    "    with FileChunkIO(source_path, 'r', offset=offset,\n",
    "                     bytes=bytes) as fp:\n",
    "        mp.upload_part_from_file(fp, part_num=i + 1)\n",
    "\n",
    "# Finish the upload\n",
    "mp.complete_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdd-damage360\n",
      "sfbayareabikeshare\n"
     ]
    }
   ],
   "source": [
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdd-damage360/classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "# for obj in s3.Bucket(name='cdd-damage360').objects.all():\n",
    "#     print(os.path.join(obj.bucket_name, obj.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# myurl = \"https://s3-us-west-2.amazonaws.com/cdd-damage360/classifier.pickle\"\n",
    "# classifier = urlopen(myurl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
